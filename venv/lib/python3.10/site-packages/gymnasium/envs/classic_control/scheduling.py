"""
Classic cart-pole system implemented by Rich Sutton et al.
Copied from http://incompleteideas.net/sutton/book/code/pole.c
permalink: https://perma.cc/C9ZM-652R
"""
import sys
sys.path.append('/etc/kubernetes') 
import math
from typing import Optional, Tuple, Union
import numpy as np
import gymnasium as gym
from gymnasium import logger, spaces
from model.main import node_count, get_nodes_network_usage, get_worker_nodes_internal_ips
import dash
from dash import dcc, html
from dash.dependencies import Input, Output
import plotly.graph_objects as go
import random
import time
import subprocess
import threading
from multiprocessing import Process
import multiprocessing
import socket
import json

def setSuggestion(agent_suggestions):
    # Configura il client socket
    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    client_socket.connect(("localhost", 8765))

    # Componi il comando e il valore per setSuggestion
    command = "setSuggestion"
    value = json.dumps(agent_suggestions)
    request = f"{command} {value}"

    # Invia la richiesta al server
    client_socket.send(request.encode("utf-8"))

    # Chiudi la connessione
    client_socket.close()

class SchedulingEnv(gym.Env[np.ndarray, Union[int, np.ndarray]]):

    metadata = {
        "render_modes": ["human", "rgb_array"],
        "render_fps": 50,
    }

    reward_value = multiprocessing.Value('i', 0)

    @classmethod
    def setReward(cls, value):
        with cls.reward_value.get_lock():
            cls.reward_value.value = value

    @classmethod
    def getReward(cls):
        with cls.reward_value.get_lock():
            return cls.reward_value.value

    def __init__(self, render_mode: Optional[str] = None):

        #GOAL: the agent must choose the node that has the lower network_usage to schedule a new pod
        #OBSERVATION : provides the actual usage of network from nodes 
        #ACTIONS: possible actions are n_nodes +1 that is dontschedule action
        #REWARD: the reward is 1 when the choosed node has memory usage that is the lower between possible usages
        #STATE: mem usages
        #self.memory_usage_metrics = None
        #self.memory_capacity_metrics = None
        #self.cpu_usage_metrics = None
        #self.cpu_capacity_metrics = None
        self.setReward(0)
        self.worker_ips = get_worker_nodes_internal_ips()
        self.num_of_nodes = None
        self.min_usage = np.zeros(node_count()-1, dtype=np.float32)
        self.max_usage = np.full(node_count()-1, float('inf'), dtype=np.float32)
        self.network_usage_metrics = None

        #da aggiungere le informazioni sulle risorse richieste dal pod da schedulare

        self.num_of_actions = None
        self.action_space = spaces.Discrete(node_count()+ 1)
        self.observation_space = spaces.Box(self.min_usage, self.max_usage,  dtype=np.float32) #values of memory usage in range [min_usage, max_usage]
        
        self.usage_treshold = 1
        self.steps_beyond_terminated = None
        self.screen = None

        app = dash.Dash(__name__)
        app.layout = html.Div([
            dcc.Graph(id='dynamic-graph'),
            dcc.Interval(
                id='interval-component',
                interval=1000, 
                n_intervals=0
            )
        ])

        #empty graph
        initial_figure = go.Figure()
        initial_figure.add_trace(go.Scatter(x=[], y=[], mode='markers', marker=dict(size=10)))

        #layout
        initial_figure.update_layout(title='Reward function', xaxis_title='#Action', yaxis_title='Reward')

        x_data = []
        y_data = []
        
        def run_dash_server():
            app.run_server(debug=True, port=8050)

        @app.callback(
            Output('dynamic-graph', 'figure'),
            [Input('interval-component', 'n_intervals')]
        )
        def update_graph(n_intervals):
            nonlocal x_data, y_data

            #update point
            new_y = self.getReward()
            new_x = len(x_data) + 1

            # Aggiorna i dati
            x_data.append(new_x)
            y_data.append(new_y)

            # Aggiorna il grafico con i nuovi dati
            #updated_figure = go.Figure(initial_figure)
            #updated_figure.update_traces(x=x_data, y=y_data)

            # Crea un oggetto Traces con una linea continua
            trace = go.Scatter(x=x_data, y=y_data, mode='lines+markers')

            # Crea un oggetto Figure con la traccia
            updated_figure = go.Figure(trace)

            # Aggiorna il layout del grafico
            updated_figure.update_layout(
                title='Reward function',
                xaxis_title='#Action',
                yaxis_title='Reward'
            )

            return updated_figure

        # Esegui l'app Dash
        #app.run_server(debug=True, port=8050)
        #subprocess.Popen(["kubectl", "port-forward", f"kube-scheduler-vbeta3-control-plane", "8083:8050", "-n", "kube-system"])
        #time.sleep(2)

        dash_process = multiprocessing.Process(target=run_dash_server)
        dash_process.start()

        self.reset()



    def reset(
        self,
        *,
        seed: Optional[int] = None,
        options: Optional[dict] = None,
    ):
        super().reset(seed=seed)
        # Note that if you use custom reset bounds, it may lead to out-of-bound
        # state/observations.
        #print(self.max_usage)
        self.state = get_nodes_network_usage(self.worker_ips)
        #print(f"RESET STATE: {self.state}")

        self.num_of_nodes = node_count()
        self.network_usage_metrics = get_nodes_network_usage([self.worker_ips])
        self.num_of_actions = node_count() + 1

        return np.array(self.state, dtype=np.float32), {}

    def step(self, action):
        assert self.action_space.contains(
            action
        ), f"{action!r} ({type(action)}) invalid"
        assert self.state is not None, "Call reset before using step method."
        
        #########
        #APPLICARE L'ACTION SUL CLUSTER REALE TIPO schedule_on_node(action)
        #########

        print(f"ACTION: {action}")

        #aggiornare lo stato
        old_state = self.state
        self.state = get_nodes_network_usage(self.worker_ips)
        #print(f"NEW STATE: {self.state}")

        #verifico se il nodo scelto era quello giusto:   
        node_to_select = -1  # Inizializza l'indice del nodo selezionato a -1 (nessun nodo selezionato)
        min_memory_usage = float('inf')     

        for i, node in enumerate(self.state):  
            network_usage = node
            if network_usage < min_memory_usage:
                min_memory_usage = network_usage
                node_to_select = i

        if node_to_select != -1:
            print(f"action: {action}   node_to_select: {node_to_select +1}")
            terminated = action.__eq__(node_to_select + 1)
            
        else:
            print("Nessun nodo con memory usage inferiore alla soglia")

        reward = 1 if terminated else 0  # Binary sparse rewards
        self.setReward(reward)
        print(f"TERMINATED: {terminated}")

        agent_suggestions = {}

        if terminated:
            node_names= self.worker_ips
            for i, node_name in enumerate(node_names):
                if i == (action - 1):
                    agent_suggestions[node_name] = 10
                else:
                    agent_suggestions[node_name] = 1

            print(f"agent_suggestions: {agent_suggestions}")
            setSuggestion(agent_suggestions)

        print(f"REWARD: {reward}")
        return np.array(self.state, dtype=np.float32), reward, terminated, False, {}

    

    def close(self):
        if self.screen is not None:
            import pygame
            pygame.display.quit()
            pygame.quit()
            self.isopen = False

            
